{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Rouge"
      ],
      "metadata": {
        "id": "-6NGVj-29Reo",
        "outputId": "9a3f153c-431e-4e31-c5b6-06ca4075d2a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from Rouge) (1.16.0)\n",
            "Installing collected packages: Rouge\n",
            "Successfully installed Rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3eqRD0k7All",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805d9ea0-4014-4d6c-b201-d4ef26ad0724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from heapq import nlargest\n",
        "from string import punctuation\n",
        "from rouge import Rouge\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"\"\"\n",
        "By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\n",
        "\"\"\"\n",
        "refSumm1 = \"\"\"\n",
        "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
        "He contracted the infection through contaminated food in Italy .\n",
        "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "text2 = \"\"\"\n",
        "A drunk driver who killed a young woman in a head-on crash while checking his mobile phone has been jailed for six years. Craig Eccleston-Todd, 27, was driving home from a night at a pub when he received a text message. As he was reading or replying to it, he veered across the road while driving round a bend and smashed into Rachel Titleyâ€™s car coming the other way. Craig Eccleston-Todd, 27 (left) was using his mobile phone when he crashed head-on into the car being driven by Rachel Titley, 28 (right). She died later from her injuries . The head-on crash took place in October 2013. Mr Eccleston-Todd's car was barely recognisable (pictured) Police said Eccleston-Todd had drunk at least three or four pints of beer before getting behind the wheel. He was found guilty of causing death by dangerous driving at Portsmouth Crown Court yesterday. Miss Titley, a 28-year-old solicitorâ€™s clerk from Cowes, Isle of Wight, had also spent the evening with friends at a pub but had not drunk any alcohol, police said. She was driving responsibly and there was â€˜nothing she could have done to avoid the collisionâ€™, they added. Lindsay Pennell, prosecuting, said: â€˜Craig Eccleston-Toddâ€™s driving resulted in the tragic death of a young woman, Rachel Titley, a death that could have been avoided. â€˜Mr Eccleston-Todd took the decision to pick up his mobile phone whilst driving and, either reading or replying to this text message, was so distracted that he failed to negotiate a left-hand bend, crossing the central white line into the path of Miss Titleyâ€™s oncoming car. Miss Titley was pulled the wreckage of herÂ Daihatsu Cuore but died later from her injuries in hospital . â€˜Miss Titley [had] a bright future ahead of her. She was also returning home having spent an enjoyable evening with friends and was driving responsibly. â€˜She had arranged to contact her friends when she got home to confirm that she had arrived safely. Her friends sadly never heard from her after they parted company. â€˜Miss Titleyâ€™s death in these circumstances reiterates the danger of using a hand-held mobile phone whilst driving.â€™ Police were unable to take breath or blood tests from Eccleston-Todd immediately, but in tests several hours after the accident he was only marginally under the drink-drive limit. The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titleyâ€™s blue Daihatsu Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His phone records showed he was also texting around the time of the crash. PC Mark Furse, from Hampshire constabularyâ€™s serious collision investigation unit, said: 'Our thoughts are with Rachel's family at this time. She had been out with friends at a pub in Shalfleet that evening, but had not had any alcohol. 'Our investigation showed that there was nothing she could have done to avoid the collision and sadly it cost her her life. 'Mr Eccleston-Todd had left work in Yarmouth and met with friends at a pub where he drank at least three to four pints of lager. He hadn't long left the pub to return home when the collision occurred at around 9.30pm. 'We weren't able to take breath or blood tests from him immediately and although blood taken several hours after the collision showed he was marginally under the limit, we maintain he would have been over the limit at the time of the collision and in summing up today, the judge agreed. 'The analysis of his phone records showed that he was texting on his phone around the time of the collision so it's highly likely this would also have contributed to his dangerous driving and loss of control.' Eccleston-Todd was found guilty of causing death by dangerous driving following a trial at Portsmouth Crown Court (pictured) He added: 'Mr Eccleston-Todd will now spend six years behind bars, but Rachel's family have lost her forever. 'I hope this will make people think twice before drinking any alcohol and getting behind the wheel, or using a phone once they're on the road. 'The dangers of drink driving and driving whilst using a mobile phone are obvious. Those who continue to do so risk spending a substantial time in prison. This case highlights just how tragic the consequences of committing these offences can be.' â€˜Mr Eccleston-Todd will now spend six years behind bars, but Rachelâ€™s family have lost her for ever. I hope this will make people think twice before drinking any alcohol and getting behind the wheel, or using a phone once theyâ€™re on the road. This case highlights just how tragic the consequences of committing these offences can be.â€™ Eccleston-Todd, of Newport, Isle of Wight, was also disqualified from driving for eight yearsÂ after which he will have to complete an extended re-test.\n",
        "\"\"\"\n",
        "refSumm2 = \"\"\"\n",
        "Craig Eccleston-Todd, 27, had drunk at least three pints before driving car .\n",
        "Was using phone when he veered across road in Yarmouth, Isle of Wight .\n",
        "Crashed head-on into 28-year-old Rachel Titley's car, who died in hospital .\n",
        "Police say he would have been over legal drink-drive limit at time of crash .\n",
        "He was found guilty at Portsmouth Crown Court of causing death by dangerous driving .\n",
        "\"\"\"\n",
        "\n",
        "text3 = \"\"\"\n",
        "Fleetwood are the only team still to have a 100% record in Sky Bet League One as a 2-0 win over Scunthorpe sent Graham Alexanderâ€™s men top of the table. The Cod Army are playing in the third tier for the first time in their history after six promotions in nine years and their remarkable ascent shows no sign of slowing with Jamie Proctor and Gareth Evans scoring the goals at Glanford Park. Fleetwood were one of five teams to have won two out of two but the other four clubs - Peterborough, Bristol City, Chesterfield and Crawley - all hit their first stumbling blocks. Posh were defeated 2-1 by Sheffield United, who had lost both of their opening contests. Jose Baxterâ€™s opener gave the Blades a first-half lead, and although it was later cancelled out by Shaun Brisleyâ€™s goal, Ben Davies snatched a winner six minutes from time. In the lead: Jose Baxter (right) celebrates opening the scoring for Sheffield United . Up for the battle: Sheffield United's Michael Doyle (left) challenges Peterborough's Kyle Vassell in a keenly-contested clash . Bristol City, who beat Nigel Cloughâ€™s men on the opening day, were held to a goalless draw by last season's play-off finalists Leyton Orient while Chesterfield, the League Two champions, were beaten 1-0 by MK Dons, who play Manchester United in the Capital One Cup in seven daysâ€™ time. Arsenal loanee Benik Afobe scored the only goal of the game just after the break. Meanwhile, Crawley lost their unbeaten status, while Bradford maintained theirs, thanks to a 3-1 win for the Bantams. James Hanson became the first player to score against Crawley this season after 49 minutes before Joe Walsh equalised five minutes later. Heads up: Bristol City's Korey Smith (left) and Leyton Orient's Lloyd James go up for a header . But strikes from Billy Knott and Mason Bennett sealed an impressive away win Phil Parkinson's men. Bradford are now second behind Fleetwood after Doncasterâ€™s stoppage-time equaliser meant Preston, for whom Joe Garner signed a new contract earlier on Tuesday, were held to a 1-1 draw which slipped them down the table. Chris Humphrey looked to have secured the points for the Lilywhites but Nathan Tyson struck a last-gasp leveller. Stand-in striker Matt Done scored a hat-trick for Rochdale in the eveningâ€™s high-scoring affair as Crewe were hammered 5-2. Marcus Haber marked his full Railwaymen debut with a brace but Doneâ€™s treble and goals from Ian Henderson and Peter Vincenti helped Keith Hillâ€™s men to a big away victory. There were plenty of goals between Coventry and Barnsley too in a 2-2 draw with all four goals coming in the first half. Josh McQuoid and Jordan Clarke twice gave the Sky Blues the lead, but the Tykes earned a point thanks to strikes from Conor Hourihane and Leroy Lita. Notts County recorded a 2-1 home win over Colchester with Ronan Murray and Liam Noble on target. Freddie Sears replied for Colchester. James Wilson's second half equaliser earned Oldham a points against Port Vale after Tom Pope's opener and Yeovil claimed a 2-1 away victory at Walsall with Kevin Dawson striking a late winner. Tom Bradshaw had equalised after veteran James Hayter gave the Glovers the lead. Finally, Swindon held Gillingham to a 2-2 draw thanks to Stephen Bywaterâ€™s last-minute own goal. Danny Kedwell and Kortney Hause twice gave the Gills the lead but Andy Williams pulled Swindon level before Bywater dropped Raphael Branco's cross into his own net.\n",
        "\"\"\"\n",
        "refSumm3 = \"\"\"\n",
        "Fleetwood top of League One after 2-0 win at Scunthorpe .\n",
        "Peterborough, Bristol City, Chesterfield and Crawley all drop first points of the season .\n",
        "Stand-in striker Matt Done scores a hat-trick as Rochdale thrash Crewe 5-2 .\n",
        "Wins for Notts County and Yeovil .\n",
        "Coventry/Bradford and Oldham/Port Vale both end in draws .\n",
        "A late Stephen Bywater own goal denies Gillingham three points against Millwall .\n",
        "\"\"\"\n",
        "\n",
        "article = []\n",
        "refehighlights = []\n",
        "originalLen = []\n",
        "newLenTfiDF = []\n",
        "newLenBOW = []\n",
        "\n",
        "# Sample array to hold all the article\n",
        "article.append(text1)\n",
        "article.append(text2)\n",
        "article.append(text3)\n",
        "\n",
        "# Sample array to hold the original length of the article\n",
        "originalLen.append(len(text1))\n",
        "originalLen.append(len(text2))\n",
        "originalLen.append(len(text3))\n",
        "\n",
        "\n",
        "# Sample reference array to hold all the heighlights \n",
        "refehighlights.append(refSumm1)\n",
        "refehighlights.append(refSumm1)\n",
        "refehighlights.append(refSumm1)\n",
        "\n",
        "# Sample array to hold the generated array from TFIDF\n",
        "generatedArrayTFIDF = []\n",
        "# Sample array to hold the generated array from BOW\n",
        "generatedArrayBOW = []\n"
      ],
      "metadata": {
        "id": "8jze8i_M9gK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = list(STOP_WORDS)\n",
        "punctuation = punctuation + '\\n'\n",
        "\n",
        "print(stopwords)\n",
        "print(punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAbTwVCA_aJE",
        "outputId": "3cb3a2bd-c0c5-46a5-d5a5-31b526c2cd4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['get', 'either', 'during', 'once', 'becoming', 'whereafter', 'wherever', 'two', 'whereby', '‘d', 'nevertheless', 'least', 'go', 're', 'amount', 'in', 'further', 'among', 'am', 'hundred', 'each', 'should', 'nobody', 'together', 'forty', 'name', '‘re', 'not', 'what', 'there', 'made', 'many', 'itself', 'formerly', 'off', 'move', 'whenever', 'of', 'that', 'front', '‘ve', 'cannot', 'hers', 'then', 'own', 'put', 'beyond', 'no', 'via', 'n‘t', 'others', 'whom', 'one', 'see', 'indeed', 'therefore', 'eight', 'become', 'side', 'almost', 'without', 'did', 'nine', 'and', 'may', 'against', 'anything', 'thereafter', 'which', 'up', 'became', 'thereupon', \"n't\", 'she', 'twenty', 'until', 'him', 'around', 'any', 'whose', 'six', 'by', 'yourselves', 'i', 'ours', 'fifty', '’ve', 'fifteen', 'again', 'everywhere', 'when', 'namely', 'used', 'something', 'nor', 'about', 'afterwards', 'amongst', '’d', 'who', 'four', 'now', 'same', 'using', 'twelve', 'latterly', 'else', 'all', 'everything', 'thus', 'always', 'within', 'into', 'he', 'very', 'yourself', 'from', 'wherein', 'herself', 'somewhere', 'due', 'quite', 'have', 'myself', 'their', 'over', 'whether', 'nowhere', 'onto', 'its', 'latter', 'thru', 'if', 'just', 'them', 'her', 'where', 'will', 'before', 'as', 'an', 'along', 'various', 'between', 'to', 'empty', 'n’t', 'seeming', 'be', '’s', 'on', 'the', 'it', 'keep', 'our', 'at', 'can', 'enough', 'already', 'ourselves', 'might', 'several', \"'d\", 'sometime', 'through', 'since', 'give', 'was', 'five', 'both', 'neither', 'bottom', 'back', 'are', 'under', \"'m\", 'done', 'hence', 'much', 'so', 'after', 'sixty', '‘s', 'you', 'another', 'themselves', 'meanwhile', 'toward', 'is', 'behind', 'these', 'out', 'whoever', 'some', 'part', 'were', 'anyway', 'doing', 'with', 'whither', 'few', '‘m', 'nothing', 'would', 'never', 'below', 'ca', 'how', 'seems', 'noone', 'somehow', '’ll', 'because', '’m', 'for', 'alone', 'every', 'moreover', 'therein', 'unless', 'us', 'often', 'beside', 'rather', 'anywhere', 'anyhow', 'your', 'this', 'why', 'besides', 'such', 'or', 'only', 'do', 'sometimes', 'top', 'yet', 'show', \"'s\", 'former', 'ever', 'however', 'hereupon', 'whence', 'must', 'upon', 'above', 'most', 'been', 'call', 'first', 'mostly', 'elsewhere', 'regarding', 'though', 'herein', 'here', 'his', 'full', 'eleven', 'seem', 'thereby', 'please', 'me', 'does', 'perhaps', 'also', 'those', 'but', 'last', 'although', 'take', 'becomes', 'could', 'still', 'say', 'whereas', 'otherwise', 'a', 'had', 'than', \"'re\", 'more', 'less', 'yours', 'someone', 'throughout', 'hereby', 'three', 'whatever', 'down', 'make', 'has', 'beforehand', 'himself', 'none', 'whole', 'across', 'they', 'even', 'hereafter', 'thence', 'everyone', 'third', 'next', 'my', 'we', '’re', \"'ve\", 'towards', 'serious', 'really', 'whereupon', 'mine', 'ten', 'per', \"'ll\", 'except', 'while', '‘ll', 'being', 'other', 'seemed', 'too', 'anyone', 'well']\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "ql3OGTqADV0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Summary\n",
        "# Clearing the array everytime it runs\n",
        "generatedArrayTFIDF = []\n",
        "newLenTfiDF = []\n",
        "i = 0\n",
        "while(i<len(article)):\n",
        "  sentences = article[i].split(\".\")\n",
        "  # Create a TfidfVectorizer\n",
        "  vectorizer = TfidfVectorizer()\n",
        "\n",
        "  # Transform the sentences into a TF-IDF representation\n",
        "  tfidf = vectorizer.fit_transform(sentences)\n",
        "  feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "  # Find the n most important sentences\n",
        "  n = 2\n",
        "  indices = np.argsort(tfidf.toarray().sum(axis=1))[-n:]\n",
        "  most_important_sentences = [sentences[i] for i in indices]\n",
        "\n",
        "  # Generate the text summary\n",
        "  summary = \" \".join(most_important_sentences) + \".\"\n",
        "  generatedArrayTFIDF.append(summary)\n",
        "  newLenTfiDF.append(len(summary))\n",
        "  print(summary)\n",
        "  print(\"------------------------------------------------------------------------------\")\n",
        "\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "zEWahpTyCIYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b8c1cd1-6295-48cb-ee9a-0172673322e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A   The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October.\n",
            "------------------------------------------------------------------------------\n",
            " 'We weren't able to take breath or blood tests from him immediately and although blood taken several hours after the collision showed he was marginally under the limit, we maintain he would have been over the limit at the time of the collision and in summing up today, the judge agreed  â€˜Mr Eccleston-Todd took the decision to pick up his mobile phone whilst driving and, either reading or replying to this text message, was so distracted that he failed to negotiate a left-hand bend, crossing the central white line into the path of Miss Titleyâ€™s oncoming car.\n",
            "------------------------------------------------------------------------------\n",
            " The Cod Army are playing in the third tier for the first time in their history after six promotions in nine years and their remarkable ascent shows no sign of slowing with Jamie Proctor and Gareth Evans scoring the goals at Glanford Park  Bristol City, who beat Nigel Cloughâ€™s men on the opening day, were held to a goalless draw by last season's play-off finalists Leyton Orient while Chesterfield, the League Two champions, were beaten 1-0 by MK Dons, who play Manchester United in the Capital One Cup in seven daysâ€™ time.\n",
            "------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generatedArrayTFIDF"
      ],
      "metadata": {
        "id": "y5HbXIkPGIjK",
        "outputId": "8f4dd4e5-ce49-4592-e8b5-2e47f268a24a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A   The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October.',\n",
              " \" 'We weren't able to take breath or blood tests from him immediately and although blood taken several hours after the collision showed he was marginally under the limit, we maintain he would have been over the limit at the time of the collision and in summing up today, the judge agreed  â€˜Mr Eccleston-Todd took the decision to pick up his mobile phone whilst driving and, either reading or replying to this text message, was so distracted that he failed to negotiate a left-hand bend, crossing the central white line into the path of Miss Titleyâ€™s oncoming car.\",\n",
              " \" The Cod Army are playing in the third tier for the first time in their history after six promotions in nine years and their remarkable ascent shows no sign of slowing with Jamie Proctor and Gareth Evans scoring the goals at Glanford Park  Bristol City, who beat Nigel Cloughâ€™s men on the opening day, were held to a goalless draw by last season's play-off finalists Leyton Orient while Chesterfield, the League Two champions, were beaten 1-0 by MK Dons, who play Manchester United in the Capital One Cup in seven daysâ€™ time.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of Original Text and Summary:\n",
        "i = 0\n",
        "while i<len(article):\n",
        "  print(\"Original Text: \",originalLen[i],\", Summary: \",newLenTfiDF[i])\n",
        "  i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAjNZi17g4yv",
        "outputId": "072dfd2e-928c-40df-dfea-d1fda1439b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:  1213 , Summary:  389\n",
            "Original Text:  4785 , Summary:  566\n",
            "Original Text:  3461 , Summary:  529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rouge for TFIDF"
      ],
      "metadata": {
        "id": "Z8rbGiylilr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "\n",
        "print(\"Rouge scores for TFIDF\")\n",
        "print()\n",
        "print()\n",
        "\n",
        "i = 0\n",
        "while(i<len(generatedArrayTFIDF)):\n",
        "  generated_summ = generatedArrayTFIDF[i]\n",
        "  reference_summ = refehighlights[i]\n",
        "\n",
        "  scores = rouge.get_scores(generated_summ,reference_summ,avg = True)\n",
        "  rouge_1 = scores['rouge-1']['f']\n",
        "  rouge_2 = scores['rouge-2']['f']\n",
        "  rouge_l = scores['rouge-l']['f']\n",
        "\n",
        "  # Print the ROUGE scores\n",
        "  print(f\"ROUGE-1: {rouge_1:.2f}\")\n",
        "  print(f\"ROUGE-2: {rouge_2:.2f}\")\n",
        "  print(f\"ROUGE-L: {rouge_l:.2f}\")\n",
        "\n",
        "  print()\n",
        "  print(\"---------------------------------------------------------\")\n",
        "  print()\n",
        "  i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPwhMvPCilVW",
        "outputId": "ec4c65ec-4374-4aa0-c31e-2e2446c1288f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rouge scores for TFIDF\n",
            "\n",
            "\n",
            "ROUGE-1: 0.39\n",
            "ROUGE-2: 0.19\n",
            "ROUGE-L: 0.39\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "ROUGE-1: 0.14\n",
            "ROUGE-2: 0.02\n",
            "ROUGE-L: 0.11\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "ROUGE-1: 0.11\n",
            "ROUGE-2: 0.00\n",
            "ROUGE-L: 0.09\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOW"
      ],
      "metadata": {
        "id": "ET6dPQwEXi0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# Clearing the array everytime it runs\n",
        "generatedArrayBOW = []\n",
        "newLenBOW = []\n",
        "\n",
        "# Create the bag-of-words model\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "\n",
        "i = 0\n",
        "while(i<len(article)):\n",
        "  sentences = article[i].split(\".\")\n",
        "  \n",
        "  # Transform the sentences to vectors\n",
        "  sentence_vectors = vectorizer.fit_transform(sentences)\n",
        "  \n",
        "  # Calculate the score for each sentence\n",
        "  scores = sentence_vectors.sum(axis=1)\n",
        "\n",
        "  # Normalize the scores\n",
        "  normalized_scores = scores / sentence_vectors.sum(axis=1).sum()\n",
        "\n",
        "  n = 2\n",
        "  indices = np.argsort(sentence_vectors.toarray().sum(axis=1))[-n:]\n",
        "  imp_sen = [sentences[k] for k in indices]\n",
        "\n",
        "  summary = \" \".join(imp_sen) + \".\"\n",
        "  print(summary)\n",
        "  generatedArrayBOW.append(summary)\n",
        "  newLenBOW.append(len(summary))\n",
        "\n",
        "  \n",
        "  print(\"------------------------------------------------------------------------------\")\n",
        "  i+=1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMfSM88TGPv_",
        "outputId": "49f023fa-8453-4c3e-8ba7-d889ee1477df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A   The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October.\n",
            "------------------------------------------------------------------------------\n",
            " â€˜Mr Eccleston-Todd took the decision to pick up his mobile phone whilst driving and, either reading or replying to this text message, was so distracted that he failed to negotiate a left-hand bend, crossing the central white line into the path of Miss Titleyâ€™s oncoming car  'We weren't able to take breath or blood tests from him immediately and although blood taken several hours after the collision showed he was marginally under the limit, we maintain he would have been over the limit at the time of the collision and in summing up today, the judge agreed.\n",
            "------------------------------------------------------------------------------\n",
            " The Cod Army are playing in the third tier for the first time in their history after six promotions in nine years and their remarkable ascent shows no sign of slowing with Jamie Proctor and Gareth Evans scoring the goals at Glanford Park  Bristol City, who beat Nigel Cloughâ€™s men on the opening day, were held to a goalless draw by last season's play-off finalists Leyton Orient while Chesterfield, the League Two champions, were beaten 1-0 by MK Dons, who play Manchester United in the Capital One Cup in seven daysâ€™ time.\n",
            "------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generatedArrayBOW"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79GvaDQJa5Pp",
        "outputId": "62d25439-b868-48d4-ef75-b5ffe070be89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A   The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October.',\n",
              " \" â€˜Mr Eccleston-Todd took the decision to pick up his mobile phone whilst driving and, either reading or replying to this text message, was so distracted that he failed to negotiate a left-hand bend, crossing the central white line into the path of Miss Titleyâ€™s oncoming car  'We weren't able to take breath or blood tests from him immediately and although blood taken several hours after the collision showed he was marginally under the limit, we maintain he would have been over the limit at the time of the collision and in summing up today, the judge agreed.\",\n",
              " \" The Cod Army are playing in the third tier for the first time in their history after six promotions in nine years and their remarkable ascent shows no sign of slowing with Jamie Proctor and Gareth Evans scoring the goals at Glanford Park  Bristol City, who beat Nigel Cloughâ€™s men on the opening day, were held to a goalless draw by last season's play-off finalists Leyton Orient while Chesterfield, the League Two champions, were beaten 1-0 by MK Dons, who play Manchester United in the Capital One Cup in seven daysâ€™ time.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of Original Text and Summary:\n",
        "i = 0\n",
        "while i<len(article):\n",
        "  print(\"Original Text: \",originalLen[i],\", Summary: \",newLenBOW[i])\n",
        "  i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwv0t9VGi2SA",
        "outputId": "803b1843-9b12-4051-c7f6-24e2ce9732e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:  1213 , Summary:  389\n",
            "Original Text:  4785 , Summary:  566\n",
            "Original Text:  3461 , Summary:  529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rougr for BOW"
      ],
      "metadata": {
        "id": "_g25o427f2cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = Rouge()\n",
        "\n",
        "print(\"Rouge scores for BOW\")\n",
        "print()\n",
        "print()\n",
        "\n",
        "i = 0\n",
        "while(i<len(generatedArrayBOW)):\n",
        "  generated_summ = generatedArrayBOW[i]\n",
        "  reference_summ = refehighlights[i]\n",
        "\n",
        "  scores = rouge.get_scores(generated_summ,reference_summ,avg = True)\n",
        "  rouge_1 = scores['rouge-1']['f']\n",
        "  rouge_2 = scores['rouge-2']['f']\n",
        "  rouge_l = scores['rouge-l']['f']\n",
        "\n",
        "  # Print the ROUGE scores\n",
        "  print(f\"ROUGE-1: {rouge_1:.2f}\")\n",
        "  print(f\"ROUGE-2: {rouge_2:.2f}\")\n",
        "  print(f\"ROUGE-L: {rouge_l:.2f}\")\n",
        "\n",
        "  print()\n",
        "  print(\"---------------------------------------------------------\")\n",
        "  print()\n",
        "  i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPQ4iPcif1dl",
        "outputId": "7e931453-d225-49ca-b2c6-3cc2536de2f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rouge scores for BOW\n",
            "\n",
            "\n",
            "ROUGE-1: 0.39\n",
            "ROUGE-2: 0.19\n",
            "ROUGE-L: 0.39\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "ROUGE-1: 0.14\n",
            "ROUGE-2: 0.02\n",
            "ROUGE-L: 0.13\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "ROUGE-1: 0.11\n",
            "ROUGE-2: 0.00\n",
            "ROUGE-L: 0.09\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network Approach"
      ],
      "metadata": {
        "id": "OXRpB_xIrj0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Example input document and summary\n",
        "document = \"This is an example input document. It contains multiple sentences and words.\"\n",
        "summary = \"This is an example summary.\"\n",
        "\n",
        "# Preprocess the input document and summary\n",
        "document_tokens = document.split()\n",
        "summary_tokens = summary.split()\n",
        "\n",
        "# Create a vocabulary from the input document and summary\n",
        "vocab = set(document_tokens + summary_tokens)\n",
        "vocab = list(vocab)\n",
        "\n",
        "\n",
        "lookUpTable = tf.lookup.StaticVocabularyTable(\n",
        "    tf.lookup.KeyValueTensorInitializer(vocab, tf.cast(tf.range(len(vocab)),tf.int64)),\n",
        "    num_oov_buckets=1)\n",
        "\n",
        "# Define the neural network architecture\n",
        "max_len = max(len(document_tokens), len(summary_tokens))\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(vocab), 64, input_length=max_len),\n",
        "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(len(document_tokens), activation='softmax')\n",
        "])\n",
        "# model = tf.keras.Sequential()\n",
        "# model.add(tf.keras.layers.Embedding(len(vocab), 64, input_length=len(document_tokens)))\n",
        "# model.add(tf.keras.layers.Flatten())\n",
        "# model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "# model.add(tf.keras.layers.Dense(len(document_tokens), activation='softmax'))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Convert the input document and summary into tensors\n",
        "document_tensor = tf.constant([document_tokens])\n",
        "summary_tensor = tf.constant([summary_tokens])\n",
        "\n",
        "# Convert the tokens into one-hot encodings\n",
        "ids_doc = lookUpTable.lookup(document_tensor)\n",
        "ids_summ = lookUpTable.lookup(summary_tensor)\n",
        "document_one_hot = tf.one_hot(ids_doc, len(vocab), dtype=tf.int32)\n",
        "summary_one_hot = tf.one_hot(ids_summ, len(vocab), dtype=tf.int32)\n",
        "\n",
        "# Train the model\n",
        "model.fit(document_one_hot, summary_one_hot, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate the model on a test document\n",
        "test_document = \"This is a test document. It should be summarized correctly.\"\n",
        "test_document_tokens = test_document.split()\n",
        "test_tensor = tf.constant([test_document_tokens])\n",
        "ids_test = lookUpTable.lookup(test_tensor)\n",
        "test_document_one_hot = tf.one_hot(ids_test, len(vocab), dtype=tf.int32)\n",
        "predicted_summary_one_hot = model.predict(test_document_one_hot)\n",
        "\n",
        "# Convert the predicted summary back into tokens\n",
        "predicted_summary_tokens = []\n",
        "for i in range(len(predicted_summary_one_hot)):\n",
        "    predicted_summary_tokens.append(vocab[tf.argmax(predicted_summary_one_hot[i])])\n",
        "\n",
        "# Join the predicted summary tokens into a string\n",
        "predicted_summary = \" \".join(predicted_summary_tokens)\n",
        "\n",
        "# Print the predicted summary\n",
        "print(predicted_summary)"
      ],
      "metadata": {
        "id": "fTifbf0LrgYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression Approach"
      ],
      "metadata": {
        "id": "BVMXu1v6RZvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Sample input data\n",
        "documents = ['This is the first document.', 'This is the second document.', 'This is the third document.']\n",
        "summaries = ['This is a summary of the first document.', 'This is a summary of the second document.', 'This is a summary of the third document.']\n",
        "\n",
        "documents = np.array(documents)\n",
        "summaries = np.array(summaries)\n",
        "\n",
        "# Convert documents and summaries to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "Y = vectorizer.transform(summaries)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(0.8 * len(documents))\n",
        "X_train, Y_train = X[:train_size], Y[:train_size]\n",
        "X_test, Y_test = X[train_size:], Y[train_size:]\n",
        "\n",
        "# Train a linear regression model\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train, Y_train)\n",
        "\n",
        "# Predict summaries for the testing set\n",
        "Y_pred = reg.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics (e.g., mean squared error)\n",
        "mse = mean_squared_error(Y_test, Y_pred)\n",
        "\n",
        "print(\"Mean squared error: \", mse)\n"
      ],
      "metadata": {
        "id": "4fXe_IWDYs_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example input document and summary\n",
        "# document = \"This is an example input document. It contains multiple sentences and words.\"\n",
        "# summary = \"This is an example summary.\"\n",
        "\n",
        "# Preprocess the input document and summary\n",
        "# document_tokens = document.split()\n",
        "# summary_tokens = summary.split()\n",
        "# document_tokens = ['This is the first document.', 'This is the second document.', 'This is the third document.']\n",
        "document_tokens = article\n",
        "# summary_tokens = ['This is a summary of the first document.', 'This is a summary of the second document.', 'This is a summary of the third document.']\n",
        "summary_tokens = refehighlights\n",
        "# Convert the tokens into numpy arrays\n",
        "document_array = np.array(document_tokens)\n",
        "summary_array = np.array(summary_tokens)\n",
        "\n",
        "# Create a vocabulary from the input document and summary\n",
        "vocab = np.unique(np.concatenate((document_array, summary_array), axis=0))\n",
        "\n",
        "# Create a matrix of document term frequencies\n",
        "doc_term_matrix = np.zeros((len(document_tokens), len(vocab)))\n",
        "for i, token in enumerate(document_tokens):\n",
        "    for j, word in enumerate(vocab):\n",
        "        if word == token:\n",
        "            doc_term_matrix[i, j] += 1\n",
        "\n",
        "# Create a matrix of summary term frequencies\n",
        "sum_term_matrix = np.zeros((len(summary_tokens), len(vocab)))\n",
        "for i, token in enumerate(summary_tokens):\n",
        "    for j, word in enumerate(vocab):\n",
        "        if word == token:\n",
        "            sum_term_matrix[i, j] += 1\n",
        "\n",
        "# Compute the TF-IDF scores\n",
        "doc_freq = np.sum(doc_term_matrix, axis=0)\n",
        "sum_freq = np.sum(sum_term_matrix, axis=0)\n",
        "doc_term_freq = np.sum(doc_term_matrix > 0, axis=0)\n",
        "idf = np.log((len(document_tokens)+1) / (doc_term_freq + 1)) + 1\n",
        "tf_doc = np.log(doc_term_matrix + 1)\n",
        "tf_sum = np.log(sum_term_matrix + 1)\n",
        "tf_idf_doc = tf_doc * idf\n",
        "tf_idf_sum = tf_sum * idf\n",
        "\n",
        "# Train the linear regression model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X_train = tf_idf_doc\n",
        "y_train = tf_idf_sum\n",
        "\n",
        "reg = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on a test document\n",
        "test_document = \"This is a test document. It should be summarized correctly.\"\n",
        "test_document_tokens = test_document.split()\n",
        "\n",
        "test_doc_term_matrix = np.zeros((len(test_document_tokens), len(vocab)))\n",
        "for i, token in enumerate(test_document_tokens):\n",
        "    for j, word in enumerate(vocab):\n",
        "        if word == token:\n",
        "            test_doc_term_matrix[i, j] += 1\n",
        "\n",
        "tf_test_doc = np.log(test_doc_term_matrix + 1)\n",
        "tf_idf_test_doc = tf_test_doc * idf\n",
        "\n",
        "predicted_tf_idf_sum = reg.predict(tf_idf_test_doc)\n",
        "\n",
        "# Convert the predicted summary back into tokens\n",
        "predicted_summary_tokens = []\n",
        "for i in range(len(predicted_tf_idf_sum)):\n",
        "    predicted_summary_tokens.append(vocab[np.argmax(predicted_tf_idf_sum[i])])\n",
        "\n",
        "# Join the predicted summary tokens into a string\n",
        "predicted_summary = \" \".join(predicted_summary_tokens)\n",
        "\n",
        "# Print the predicted summary\n",
        "print(predicted_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKBlUrTLTXDi",
        "outputId": "f54cd414-9df8-4e77-cda0-a3e699190065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
            "He contracted the infection through contaminated food in Italy .\n",
            "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
            " \n",
            "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
            "He contracted the infection through contaminated food in Italy .\n",
            "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
            " \n",
            "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
            "He contracted the infection through contaminated food in Italy .\n",
            "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
            " \n",
            "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
            "He contracted the infection through contaminated food in Italy .\n",
            "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
            " \n",
            "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
            "He contracted the infection through contaminated food in Italy .\n",
            "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
            " \n",
            "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
            "He contracted the infection through contaminated food in Italy .\n",
            "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
            " \n",
            "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
            "He contracted the infection through contaminated food in Italy .\n",
            "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
            " \n",
            "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
            "He contracted the infection through contaminated food in Italy .\n",
            "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
            " \n",
            "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
            "He contracted the infection through contaminated food in Italy .\n",
            "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
            " \n",
            "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
            "He contracted the infection through contaminated food in Italy .\n",
            "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "# samp = []\n",
        "# samp.append(0)\n",
        "i = 0\n",
        "while(i<3):\n",
        "  document = article[i]\n",
        "  refSumm = refehighlights[i]\n",
        "\n",
        "  document_tok = document.split()\n",
        "  refSumm_tok = refSumm.split()\n",
        "\n",
        "  document_array = np.array(document_tok)\n",
        "  refSumm_array = np.array(refSumm_tok)\n",
        "\n",
        "  vocab = np.unique(np.concatenate((document_array,refSumm_array), axis=0))\n",
        "\n",
        "  # Create a matrix of document term frequencies\n",
        "  doc_term_matrix = np.zeros((len(document_tok), len(vocab)))\n",
        "  for i, token in enumerate(document_tok):\n",
        "      for j, word in enumerate(vocab):\n",
        "          if word == token:\n",
        "              doc_term_matrix[i, j] += 1\n",
        "\n",
        "  # Create a matrix of summary term frequencies\n",
        "  sum_term_matrix = np.zeros((len(refSumm_tok), len(vocab)))\n",
        "  for i, token in enumerate(refSumm_tok):\n",
        "      for j, word in enumerate(vocab):\n",
        "          if word == token:\n",
        "              sum_term_matrix[i, j] += 1\n",
        "\n",
        "  doc_freq = np.sum(doc_term_matrix,axis = 0)\n",
        "  sum_freq = np.sum(sum_term_matrix,axis = 0)\n",
        "  doc_term_freq = np.sum(doc_term_matrix > 0, axis=0)\n",
        "  idf = np.log((len(document_tok)+1) / (doc_term_freq + 1)) + 1\n",
        "  tf_doc = np.log(doc_term_matrix + 1)\n",
        "  tf_sum = np.log(sum_term_matrix + 1)\n",
        "  tf_idf_doc = tf_doc * idf\n",
        "  tf_idf_sum = tf_sum * idf\n",
        "  \n",
        "  X_train.append([tf_idf_doc])\n",
        "  Y_train.append(tf_idf_sum)\n",
        "\n",
        "  i+=1\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0],-1))\n",
        "Y_train = Y_train.reshape((Y_train.shape[0],-1))\n",
        "# Fitting the Model for X_train and Y_train\n",
        "reg = LinearRegression().fit(X_train,Y_train)\n",
        "Test_article = \"\"\"\n",
        "There are a number of job descriptions waiting for Darren Fletcher when he settles in at West Brom but the one he might not have expected is Saido Berahinoâ€™s nanny. Fletcherâ€™s unveiling as the deadline day signing from Manchester United was almost eclipsed by the 21-year-old striker, who is acquiring the habit of talking himself into trouble. Ten years Berahinoâ€™s senior, Fletcher will be expected to mentor a player who told the world this week that he wanted to play for a bigger club. Tony Pulis has advised Saido Berahino to focus on his performances at West Brom . Darren Fletcher has signed for the baggies where he will be asked to provide a role model for young players . That is off the pitch. On it, the Scotland midfielder wants to prove he is good enough to cut the mustard in the Premier League after finding starts harder and harder to come by at Old Trafford. Head coach Tony Pulis believes that Fletcher, who has agreed a three-and-a-half year contract, will be captain of Albion one day. Having checked with Sir Alex Ferguson last year when he was Crystal Palace, Pulis did not need any more due diligence before moving in when a deal with West Ham collapsed. Pulis wants Fletcher to be his voice in the dressing room, especially when it comes to the younger players who may be led astray. Berahino has caught the eye with impressive performances at West Brom and suggested he could move on . Berahinoâ€™s latest outburst this week comes after he was found guilty of drink-driving and after he moodly refused to celebrate a hat-trick against Gateshead. â€˜Things are not what they used to be,â€™ added Pulis. â€˜The mentors for these young lads are just not there. These kids need guides and mentors so that the youngsters can respect them and take notice. â€˜I think Fletch will be critical to that sort of stuff but give him time to settle in. As a character, having worked with him for a week, he is first class. He got through his illness with flying colours and I see him as a future captain of the club.â€™ As for Berahino, he will escape a fine. â€˜He's been in a naughty chair. That's in my office,â€™ joked Pulis, although the underlying message was rather more serious. â€˜We've had no phone calls. He needs to stop listening to all the kerfuffle.. This is a great football club with great players. And Saido has not become that yet. Pulis praised recent recruit Darren Fletcher and feels he could be an ideal role model for Berahino . â€˜The question was whether would he like to play in a top four team and everyone wants that. His responsibility is to work for us until that happens. â€˜I've spoken to him and his people. He has to do it rather than talk about it. That's what good players do and then clubs will be interested. He's done an interview but not for what he was supposed to be talking about.â€™ Fletcher has already been impressed by Berahino on the training ground but admitted: â€˜The lads have gone straight into him. He has said something and he will learn from it. â€˜He loves West Brom and wants to do well. Heâ€™s a young player who said something he shouldnâ€™t and he probably regrets it. Iâ€™ve done that, all young players do that. â€˜On first impressions he looks very sharp, a real goalscorer. Heâ€™s not shy!. Giving me orders straight away because he wants to score goals. Heâ€™s a nice kid welcoming, respectful and can be big influence for rest of season.â€™\n",
        "\"\"\"\n",
        "Test_article_tok = Test_article.split()\n",
        "Test_array = np.array(Test_article_tok)\n",
        "vocab = np.unique(Test_array)\n",
        "\n",
        "test_doc_term_matrix = np.zeros((len(Test_article_tok), len(vocab)))\n",
        "for i, token in enumerate(Test_article_tok):\n",
        "    for j, word in enumerate(vocab):\n",
        "        if word == token:\n",
        "            test_doc_term_matrix[i, j] += 1\n",
        "\n",
        "Test_doc_freq = np.sum(test_doc_term_matrix,axis=0)\n",
        "Test_doc_term_freq = np.sum(test_doc_term_matrix>0,axis = 0)\n",
        "idf = np.log((len(Test_article_tok)+1) / (Test_doc_term_freq + 1)) + 1\n",
        "tf_test_doc = np.log(test_doc_term_matrix + 1)\n",
        "tf_idf_test_doc = tf_test_doc * idf\n",
        "\n",
        "\n",
        "predicted_tf_idf_sum = reg.predict(tf_idf_test_doc)\n",
        "\n",
        "\n",
        "predicted_summary_tokens = []\n",
        "for i in range(len(predicted_tf_idf_sum)):\n",
        "    predicted_summary_tokens.append(vocab[np.argmax(predicted_tf_idf_sum[i])])\n",
        "\n",
        "# Join the predicted summary tokens into a string\n",
        "predicted_summary = \" \".join(predicted_summary_tokens)\n",
        "\n",
        "# Print the predicted summary\n",
        "print(predicted_summary)\n"
      ],
      "metadata": {
        "id": "zMTEb1I0jqI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Document Text Summarization Based on Multiple Linear Regression"
      ],
      "metadata": {
        "id": "_ID5sm2T74zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQxdSJSA88bw",
        "outputId": "4b06c7af-3969-44ea-cb4e-fd5ee1f89cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Not with human summaries\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "# Load documents\n",
        "doc1 = \"First document is about machine learning.\"\n",
        "doc2 = \"Second document is about natural language processing.\"\n",
        "doc3 = \"Third document is about computer vision.\"\n",
        "docs = [doc1, doc2, doc3]\n",
        "\n",
        "# Tokenize documents into sentences\n",
        "sentences = []\n",
        "for doc in docs:\n",
        "    sentences += sent_tokenize(doc)\n",
        "\n",
        "# Step 2: Feature Extraction\n",
        "# Convert sentences to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "X = X.toarray()\n",
        "\n",
        "# Step 3: Training\n",
        "# Split data into training and testing sets\n",
        "num_sentences = len(sentences)\n",
        "train_size = int(num_sentences * 0.8)\n",
        "X_train = X[:train_size]\n",
        "X_test = X[train_size:]\n",
        "# Create target summary\n",
        "target = [\"Machine learning is important.\", \"Natural language processing is interesting.\"]\n",
        "y_train = np.zeros((train_size,2))\n",
        "for i in range(train_size):\n",
        "    if \"machine learning\" in sentences[i]:\n",
        "        y_train[i,0] = 1\n",
        "    if \"natural language processing\" in sentences[i]:\n",
        "        y_train[i,1] = 1\n",
        "# Train linear regression model\n",
        "reg1 = LinearRegression().fit(X_train, y_train[:,0])\n",
        "reg2 = LinearRegression().fit(X_train, y_train[:,1])\n",
        "\n",
        "# Step 4: Testing\n",
        "# Predict summary sentences\n",
        "y_pred1 = reg1.predict(X_test)\n",
        "y_pred2 = reg2.predict(X_test)\n",
        "y_pred = np.column_stack((y_pred1, y_pred2))\n",
        "top_sentences = []\n",
        "for i in np.argsort(-y_pred.sum(axis=1))[:2]:\n",
        "    top_sentences.append(sentences[train_size + i])\n",
        "# Generate summary\n",
        "summary = \" \".join(top_sentences)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kipEvdWo74N9",
        "outputId": "848f2c60-a898-45d1-b5e6-0b084efe251a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Third document is about computer vision.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With Human summaries but also with target summary\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "# Load documents\n",
        "doc1 = \"First document is about machine learning.\"\n",
        "doc2 = \"Second document is about natural language processing.\"\n",
        "doc3 = \"Third document is about computer vision.\"\n",
        "docs = [doc1, doc2, doc3]\n",
        "\n",
        "# Tokenize documents into sentences\n",
        "sentences = []\n",
        "for doc in docs:\n",
        "    sentences += sent_tokenize(doc)\n",
        "\n",
        "# Step 2: Feature Extraction\n",
        "# Convert sentences to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "X = X.toarray()\n",
        "\n",
        "# Step 3: Training\n",
        "# Create target summary\n",
        "target = [\"Machine learning is important.\", \"Natural language processing is interesting.\"]\n",
        "y = np.zeros((len(sentences), 2))\n",
        "for i in range(len(sentences)):\n",
        "    if \"machine learning\" in sentences[i]:\n",
        "        y[i, 0] = 1\n",
        "    if \"natural language processing\" in sentences[i]:\n",
        "        y[i, 1] = 1\n",
        "# Load human-generated summaries\n",
        "human_summaries = [\n",
        "    \"Machine learning is a rapidly growing field with many exciting applications.\",\n",
        "    \"Natural language processing has the potential to revolutionize communication and information retrieval.\"\n",
        "]\n",
        "# Convert human summaries to target vectors\n",
        "human_targets = np.zeros((len(human_summaries), len(sentences)))\n",
        "for i, summary in enumerate(human_summaries):\n",
        "    summary_sentences = sent_tokenize(summary)\n",
        "    for j in range(len(sentences)):\n",
        "        if sentences[j] in summary_sentences:\n",
        "            human_targets[i, j] = 1\n",
        "\n",
        "# Transpose human_targets to match the shape of y\n",
        "human_targets = human_targets.T\n",
        "\n",
        "\n",
        "# Train linear regression model\n",
        "reg1 = LinearRegression().fit(X, human_targets[:, 0])\n",
        "reg2 = LinearRegression().fit(X, human_targets[:, 1])\n",
        "\n",
        "# Step 4: Testing\n",
        "# Predict summary sentences\n",
        "y_pred1 = reg1.predict(X)\n",
        "y_pred2 = reg2.predict(X)\n",
        "y_pred = np.column_stack((y_pred1, y_pred2))\n",
        "top_sentences = []\n",
        "for i in np.argsort(-y_pred.sum(axis=1))[:2]:\n",
        "    top_sentences.append(sentences[i])\n",
        "# Generate summary\n",
        "summary = \" \".join(top_sentences)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVqPmRg_-Oap",
        "outputId": "4af5b739-444e-4451-871b-8629be717812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First document is about machine learning. Second document is about natural language processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Without Targeted Summary\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "WU384UC-_nZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation.replace(\".\", \"\")\n",
        "punctuation += '\\n'\n",
        "print(punctuation)\n",
        "\n",
        "stop_words = list(STOP_WORDS)\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01vi2jERMS0D",
        "outputId": "934b04b3-327c-4adf-d901-1030bd977902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "['about', 'other', 'these', 'full', 'still', 'whole', 'was', 'down', 'somehow', 'eleven', 'noone', 'off', 'therefore', 'our', 'every', 'yourselves', 'whenever', 'he', '’re', 'bottom', 'nowhere', 'although', 'several', 'eight', 'latter', 'your', 'hence', 'just', 'again', 'everyone', 'go', 'whether', 'elsewhere', 'under', 'otherwise', 'towards', 'would', '’ll', 'what', 'sometimes', 'her', 'fifteen', 'might', \"'s\", 'so', 'unless', 'whereas', 'anyway', \"'m\", 'between', 'be', 'seem', 'me', 'themselves', 'even', 'but', '’m', 'the', 'where', 'toward', 'by', 'whose', 'n‘t', 'however', 'thus', 'within', 'well', 'six', 'fifty', 'rather', 'of', 'cannot', 'whoever', 'anywhere', 'ourselves', 'thereby', 'how', 'because', 'two', 'therein', 'four', 'than', 'during', '‘m', 'could', 'former', 'into', 'empty', 'himself', 'or', 'when', 'take', 'been', 'neither', 'more', 'then', 'does', 'they', \"'ve\", 'another', 'seemed', 'becomes', 'none', 'while', 'move', 'anything', 'alone', 'nor', 'against', 'next', 'beside', 'why', \"'ll\", 'whereby', 'onto', 'quite', 'n’t', 'which', 'thru', 'ever', 'five', 'below', 'also', 'first', 'many', 'less', 'perhaps', 'serious', 'same', 'become', 'moreover', 'their', 'side', 'beforehand', 'per', 'if', 'others', 'we', 'most', 'being', 'regarding', 'anyhow', 'wherein', 'herself', '‘s', 'its', 'sixty', 'mine', 'should', 'already', 'for', 'always', 'via', 'who', 'them', 'a', 'through', 'anyone', 'nine', 'among', 'seeming', 'and', 'across', 'see', 'above', 're', 'are', \"n't\", 'is', 'must', 'this', 'becoming', 'in', 'both', 'nevertheless', 'once', 'everything', 'least', 'at', 'thence', 'ten', 'nothing', 'very', 'due', 'on', 'such', 'there', 'else', 'three', 'keep', 'since', 'itself', 'nobody', 'twelve', 'she', 'last', 'not', \"'re\", 'hers', 'various', 'amongst', 'became', 'front', 'amount', 'twenty', 'made', 'had', 'my', 'never', 'seems', 'you', \"'d\", 'will', 'over', 'before', 'from', 'put', 'his', 'forty', 'up', 'call', 'yours', 'top', 'too', 'somewhere', 'enough', 'though', 'only', 'beyond', 'either', 'as', 'us', 'much', 'all', '‘re', 'now', 'own', 'him', 'almost', 'latterly', 'until', '’s', 'whatever', 'mostly', 'i', 'out', 'can', 'one', 'name', 'further', 'make', 'has', 'namely', 'using', 'give', 'with', 'something', 'show', 'indeed', 'really', 'few', 'were', 'along', '‘ve', 'did', 'here', 'used', 'may', 'am', 'part', 'sometime', 'together', 'it', '’d', 'throughout', 'afterwards', 'get', 'please', 'each', 'say', 'thereupon', 'herein', 'hereupon', '‘d', 'around', 'besides', 'yet', 'hereby', 'hundred', 'some', 'meanwhile', 'without', 'to', 'no', 'any', 'formerly', 'do', 'myself', '’ve', 'done', 'that', 'thereafter', 'those', 'everywhere', 'ours', 'third', 'an', 'after', 'upon', 'wherever', 'whither', 'hereafter', 'whom', 'except', 'whence', 'back', 'ca', 'behind', 'doing', 'often', '‘ll', 'whereafter', 'whereupon', 'yourself', 'someone', 'have']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Preprocessing\n",
        "text1 = \"\"\"\n",
        "A drunk driver who killed a young woman in a head-on crash while checking his mobile phone has been jailed for six years. Craig Eccleston-Todd, 27, was driving home from a night at a pub when he received a text message. As he was reading or replying to it, he veered across the road while driving round a bend and smashed into Rachel Titleyâ€™s car coming the other way. Craig Eccleston-Todd, 27 (left) was using his mobile phone when he crashed head-on into the car being driven by Rachel Titley, 28 (right). She died later from her injuries . The head-on crash took place in October 2013. Mr Eccleston-Todd's car was barely recognisable (pictured) Police said Eccleston-Todd had drunk at least three or four pints of beer before getting behind the wheel. He was found guilty of causing death by dangerous driving at Portsmouth Crown Court yesterday. Miss Titley, a 28-year-old solicitorâ€™s clerk from Cowes, Isle of Wight, had also spent the evening with friends at a pub but had not drunk any alcohol, police said. She was driving responsibly and there was â€˜nothing she could have done to avoid the collisionâ€™, they added. Lindsay Pennell, prosecuting, said: â€˜Craig Eccleston-Toddâ€™s driving resulted in the tragic death of a young woman, Rachel Titley, a death that could have been avoided. â€˜Mr Eccleston-Todd took the decision to pick up his mobile phone whilst driving and, either reading or replying to this text message, was so distracted that he failed to negotiate a left-hand bend, crossing the central white line into the path of Miss Titleyâ€™s oncoming car. Miss Titley was pulled the wreckage of herÂ Daihatsu Cuore but died later from her injuries in hospital . â€˜Miss Titley [had] a bright future ahead of her. She was also returning home having spent an enjoyable evening with friends and was driving responsibly. â€˜She had arranged to contact her friends when she got home to confirm that she had arrived safely. Her friends sadly never heard from her after they parted company. â€˜Miss Titleyâ€™s death in these circumstances reiterates the danger of using a hand-held mobile phone whilst driving.â€™ Police were unable to take breath or blood tests from Eccleston-Todd immediately, but in tests several hours after the accident he was only marginally under the drink-drive limit. The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titleyâ€™s blue Daihatsu Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His phone records showed he was also texting around the time of the crash. PC Mark Furse, from Hampshire constabularyâ€™s serious collision investigation unit, said: 'Our thoughts are with Rachel's family at this time. She had been out with friends at a pub in Shalfleet that evening, but had not had any alcohol. 'Our investigation showed that there was nothing she could have done to avoid the collision and sadly it cost her her life. 'Mr Eccleston-Todd had left work in Yarmouth and met with friends at a pub where he drank at least three to four pints of lager. He hadn't long left the pub to return home when the collision occurred at around 9.30pm. 'We weren't able to take breath or blood tests from him immediately and although blood taken several hours after the collision showed he was marginally under the limit, we maintain he would have been over the limit at the time of the collision and in summing up today, the judge agreed. 'The analysis of his phone records showed that he was texting on his phone around the time of the collision so it's highly likely this would also have contributed to his dangerous driving and loss of control.' Eccleston-Todd was found guilty of causing death by dangerous driving following a trial at Portsmouth Crown Court (pictured) He added: 'Mr Eccleston-Todd will now spend six years behind bars, but Rachel's family have lost her forever. 'I hope this will make people think twice before drinking any alcohol and getting behind the wheel, or using a phone once they're on the road. 'The dangers of drink driving and driving whilst using a mobile phone are obvious. Those who continue to do so risk spending a substantial time in prison. This case highlights just how tragic the consequences of committing these offences can be.' â€˜Mr Eccleston-Todd will now spend six years behind bars, but Rachelâ€™s family have lost her for ever. I hope this will make people think twice before drinking any alcohol and getting behind the wheel, or using a phone once theyâ€™re on the road. This case highlights just how tragic the consequences of committing these offences can be.â€™ Eccleston-Todd, of Newport, Isle of Wight, was also disqualified from driving for eight yearsÂ after which he will have to complete an extended re-test.\n",
        "\"\"\"\n",
        "text1 = text1.lower()\n",
        "\n",
        "final_text = \"\"\"\"\"\"\n",
        "word_token_text = word_tokenize(text1)\n",
        "for w in word_token_text:\n",
        "  if w =='.':\n",
        "    final_text += '. ' \n",
        "  elif w not in stop_words and w not in punctuation:\n",
        "    final_text += ' ' + w + ' '\n",
        "\n",
        "\n",
        "print(final_text)\n",
        "sentences = sent_tokenize(final_text)\n",
        "final_sent = []\n",
        "for sent in sentences:\n",
        "  if(sent != '.'):\n",
        "    final_sent.append(sent)\n",
        "sentences = final_sent\n",
        "print(final_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5C1i94HMnPP",
        "outputId": "ac1d13c9-8f99-48cc-f5a2-a4bf4916a118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " drunk  driver  killed  young  woman  head-on  crash  checking  mobile  phone  jailed  years .  craig  eccleston-todd  27  driving  home  night  pub  received  text  message .  reading  replying  veered  road  driving  round  bend  smashed  rachel  titleyâ€™s  car  coming  way .  craig  eccleston-todd  27  left  mobile  phone  crashed  head-on  car  driven  rachel  titley  28  right .  died  later  injuries .  head-on  crash  took  place  october  2013.  mr  eccleston-todd  car  barely  recognisable  pictured  police  said  eccleston-todd  drunk  pints  beer  getting  wheel .  found  guilty  causing  death  dangerous  driving  portsmouth  crown  court  yesterday .  miss  titley  28-year-old  solicitorâ€™s  clerk  cowes  isle  wight  spent  evening  friends  pub  drunk  alcohol  police  said .  driving  responsibly  â€˜nothing  avoid  collisionâ€™  added .  lindsay  pennell  prosecuting  said  â€˜craig  eccleston-toddâ€™s  driving  resulted  tragic  death  young  woman  rachel  titley  death  avoided .  â€˜mr  eccleston-todd  took  decision  pick  mobile  phone  whilst  driving  reading  replying  text  message  distracted  failed  negotiate  left-hand  bend  crossing  central  white  line  path  miss  titleyâ€™s  oncoming  car .  miss  titley  pulled  wreckage  herâ  daihatsu  cuore  died  later  injuries  hospital .  â€˜miss  titley  bright  future  ahead .  returning  home  having  spent  enjoyable  evening  friends  driving  responsibly .  â€˜she  arranged  contact  friends  got  home  confirm  arrived  safely .  friends  sadly  heard  parted  company .  â€˜miss  titleyâ€™s  death  circumstances  reiterates  danger  hand-held  mobile  phone  whilst  driving.â€™  police  unable  breath  blood  tests  eccleston-todd  immediately  tests  hours  accident  marginally  drink-drive  limit .  judge  agreed  police  limit  time  red  citroen  hit  miss  titleyâ€™s  blue  daihatsu  cuore  road  near  yarmouth  isle  wight  october  11  2013.  phone  records  showed  texting  time  crash .  pc  mark  furse  hampshire  constabularyâ€™s  collision  investigation  unit  said  'our  thoughts  rachel  family  time .  friends  pub  shalfleet  evening  alcohol .  'our  investigation  showed  avoid  collision  sadly  cost  life .  'mr  eccleston-todd  left  work  yarmouth  met  friends  pub  drank  pints  lager .  long  left  pub  return  home  collision  occurred  9.30pm .  'we  able  breath  blood  tests  immediately  blood  taken  hours  collision  showed  marginally  limit  maintain  limit  time  collision  summing  today  judge  agreed .  'the  analysis  phone  records  showed  texting  phone  time  collision  highly  likely  contributed  dangerous  driving  loss  control .  eccleston-todd  found  guilty  causing  death  dangerous  driving  following  trial  portsmouth  crown  court  pictured  added  'mr  eccleston-todd  spend  years  bars  rachel  family  lost  forever .  hope  people  think  twice  drinking  alcohol  getting  wheel  phone  road .  'the  dangers  drink  driving  driving  whilst  mobile  phone  obvious .  continue  risk  spending  substantial  time  prison .  case  highlights  tragic  consequences  committing  offences .  â€˜mr  eccleston-todd  spend  years  bars  rachelâ€™s  family  lost .  hope  people  think  twice  drinking  alcohol  getting  wheel  phone  theyâ€™re  road .  case  highlights  tragic  consequences  committing  offences  be.â€™  eccleston-todd  newport  isle  wight  disqualified  driving  yearsâ  complete  extended  re-test . \n",
            "[' drunk  driver  killed  young  woman  head-on  crash  checking  mobile  phone  jailed  years .', 'craig  eccleston-todd  27  driving  home  night  pub  received  text  message .', 'reading  replying  veered  road  driving  round  bend  smashed  rachel  titleyâ€™s  car  coming  way .', 'craig  eccleston-todd  27  left  mobile  phone  crashed  head-on  car  driven  rachel  titley  28  right .', 'died  later  injuries .', 'head-on  crash  took  place  october  2013.  mr  eccleston-todd  car  barely  recognisable  pictured  police  said  eccleston-todd  drunk  pints  beer  getting  wheel .', 'found  guilty  causing  death  dangerous  driving  portsmouth  crown  court  yesterday .', 'miss  titley  28-year-old  solicitorâ€™s  clerk  cowes  isle  wight  spent  evening  friends  pub  drunk  alcohol  police  said .', 'driving  responsibly  â€˜nothing  avoid  collisionâ€™  added .', 'lindsay  pennell  prosecuting  said  â€˜craig  eccleston-toddâ€™s  driving  resulted  tragic  death  young  woman  rachel  titley  death  avoided .', 'â€˜mr  eccleston-todd  took  decision  pick  mobile  phone  whilst  driving  reading  replying  text  message  distracted  failed  negotiate  left-hand  bend  crossing  central  white  line  path  miss  titleyâ€™s  oncoming  car .', 'miss  titley  pulled  wreckage  herâ  daihatsu  cuore  died  later  injuries  hospital .', 'â€˜miss  titley  bright  future  ahead .', 'returning  home  having  spent  enjoyable  evening  friends  driving  responsibly .', 'â€˜she  arranged  contact  friends  got  home  confirm  arrived  safely .', 'friends  sadly  heard  parted  company .', 'â€˜miss  titleyâ€™s  death  circumstances  reiterates  danger  hand-held  mobile  phone  whilst  driving.â€™  police  unable  breath  blood  tests  eccleston-todd  immediately  tests  hours  accident  marginally  drink-drive  limit .', 'judge  agreed  police  limit  time  red  citroen  hit  miss  titleyâ€™s  blue  daihatsu  cuore  road  near  yarmouth  isle  wight  october  11  2013.  phone  records  showed  texting  time  crash .', \"pc  mark  furse  hampshire  constabularyâ€™s  collision  investigation  unit  said  'our  thoughts  rachel  family  time .\", 'friends  pub  shalfleet  evening  alcohol .', \"'our  investigation  showed  avoid  collision  sadly  cost  life .\", \"'mr  eccleston-todd  left  work  yarmouth  met  friends  pub  drank  pints  lager .\", 'long  left  pub  return  home  collision  occurred  9.30pm .', \"'we  able  breath  blood  tests  immediately  blood  taken  hours  collision  showed  marginally  limit  maintain  limit  time  collision  summing  today  judge  agreed .\", \"'the  analysis  phone  records  showed  texting  phone  time  collision  highly  likely  contributed  dangerous  driving  loss  control .\", \"eccleston-todd  found  guilty  causing  death  dangerous  driving  following  trial  portsmouth  crown  court  pictured  added  'mr  eccleston-todd  spend  years  bars  rachel  family  lost  forever .\", 'hope  people  think  twice  drinking  alcohol  getting  wheel  phone  road .', \"'the  dangers  drink  driving  driving  whilst  mobile  phone  obvious .\", 'continue  risk  spending  substantial  time  prison .', 'case  highlights  tragic  consequences  committing  offences .', 'â€˜mr  eccleston-todd  spend  years  bars  rachelâ€™s  family  lost .', 'hope  people  think  twice  drinking  alcohol  getting  wheel  phone  theyâ€™re  road .', 'case  highlights  tragic  consequences  committing  offences  be.â€™  eccleston-todd  newport  isle  wight  disqualified  driving  yearsâ  complete  extended  re-test .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Feature Extraction\n",
        "# Convert sentences to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "X = X.toarray()\n",
        "\n",
        "print(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt53fHZPCQrW",
        "outputId": "382d1c0b-4a44-4a44-a975-ffec8ff3a4d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         ... 0.         0.         0.28564934]\n",
            " [0.         0.         0.33643458 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.26066383 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To do\n",
        "# Make target array based on words in sentence not the whole sentenc.\n",
        "\n",
        "# Step 3: Training\n",
        "# Load human-generated summary\n",
        "human_summary = \"\"\"Craig Eccleston-Todd, 27, had drunk at least three pints before driving car .\n",
        "Was using phone when he veered across road in Yarmouth, Isle of Wight .\n",
        "Crashed head-on into 28-year-old Rachel Titley's car, who died in hospital .\n",
        "Police say he would have been over legal drink-drive limit at time of crash .\n",
        "He was found guilty at Portsmouth Crown Court of causing death by dangerous driving .\"\"\"\n",
        "# Convert human summary to target vector\n",
        "target = np.zeros(len(sentences))\n",
        "summary_sentences = sent_tokenize(human_summary)\n",
        "for i in range(len(sentences)):\n",
        "    if sentences[i] in summary_sentences:\n",
        "      # print(\"Hello\")\n",
        "      target[i] = 1\n",
        "# Train linear regression model\n",
        "reg = LinearRegression().fit(X, target)\n",
        "\n",
        "\n",
        "print(target)\n",
        "print(len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZo9c0UmCWIC",
        "outputId": "6e8ca409-a59d-4274-c5b8-0fe6058af246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Testing\n",
        "# Predict summary sentences\n",
        "y_pred = reg.predict(X)\n",
        "top_sentences = []\n",
        "summ_len = int(len(sentences)*.4)\n",
        "for i in np.argsort(-y_pred)[:summ_len]:\n",
        "    top_sentences.append(sentences[i])\n",
        "# Generate summary\n",
        "summary = \" \".join(top_sentences)\n",
        "print(len(top_sentences))\n",
        "print(top_sentences)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glFJFG9ACZXz",
        "outputId": "4dc37be5-b9ba-4957-f02d-058d61c816c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "[' drunk  driver  killed  young  woman  head-on  crash  checking  mobile  phone  jailed  years .', 'â€˜mr  eccleston-todd  spend  years  bars  rachelâ€™s  family  lost .', 'case  highlights  tragic  consequences  committing  offences .', 'continue  risk  spending  substantial  time  prison .', \"'the  dangers  drink  driving  driving  whilst  mobile  phone  obvious .\", 'hope  people  think  twice  drinking  alcohol  getting  wheel  phone  road .', \"eccleston-todd  found  guilty  causing  death  dangerous  driving  following  trial  portsmouth  crown  court  pictured  added  'mr  eccleston-todd  spend  years  bars  rachel  family  lost  forever .\", \"'the  analysis  phone  records  showed  texting  phone  time  collision  highly  likely  contributed  dangerous  driving  loss  control .\", \"'we  able  breath  blood  tests  immediately  blood  taken  hours  collision  showed  marginally  limit  maintain  limit  time  collision  summing  today  judge  agreed .\", 'long  left  pub  return  home  collision  occurred  9.30pm .', \"'mr  eccleston-todd  left  work  yarmouth  met  friends  pub  drank  pints  lager .\", \"'our  investigation  showed  avoid  collision  sadly  cost  life .\", 'friends  pub  shalfleet  evening  alcohol .']\n",
            " drunk  driver  killed  young  woman  head-on  crash  checking  mobile  phone  jailed  years . â€˜mr  eccleston-todd  spend  years  bars  rachelâ€™s  family  lost . case  highlights  tragic  consequences  committing  offences . continue  risk  spending  substantial  time  prison . 'the  dangers  drink  driving  driving  whilst  mobile  phone  obvious . hope  people  think  twice  drinking  alcohol  getting  wheel  phone  road . eccleston-todd  found  guilty  causing  death  dangerous  driving  following  trial  portsmouth  crown  court  pictured  added  'mr  eccleston-todd  spend  years  bars  rachel  family  lost  forever . 'the  analysis  phone  records  showed  texting  phone  time  collision  highly  likely  contributed  dangerous  driving  loss  control . 'we  able  breath  blood  tests  immediately  blood  taken  hours  collision  showed  marginally  limit  maintain  limit  time  collision  summing  today  judge  agreed . long  left  pub  return  home  collision  occurred  9.30pm . 'mr  eccleston-todd  left  work  yarmouth  met  friends  pub  drank  pints  lager . 'our  investigation  showed  avoid  collision  sadly  cost  life . friends  pub  shalfleet  evening  alcohol .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple Linear Regreesion without targeted Y (Not of any use)"
      ],
      "metadata": {
        "id": "2TEmMUMeCayE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "# Load documents\n",
        "doc1 = \"First document is about machine learning.\"\n",
        "doc2 = \"Second document is about natural language processing.\"\n",
        "doc3 = \"Third document is about computer vision.\"\n",
        "docs = [doc1, doc2, doc3]\n",
        "\n",
        "# Tokenize documents into sentences\n",
        "sentences = []\n",
        "for doc in docs:\n",
        "    sentences += sent_tokenize(doc)\n",
        "\n",
        "# Step 2: Feature Extraction\n",
        "# Convert sentences to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "X = X.toarray()\n",
        "\n",
        "# Step 3: Training\n",
        "# Load human-generated summary\n",
        "human_summary = \"Machine learning is a rapidly growing field with many exciting applications.\"\n",
        "# Convert human summary to target vector\n",
        "target = np.zeros(len(sentences))\n",
        "summary_sentences = sent_tokenize(human_summary)\n",
        "for i in range(len(sentences)):\n",
        "    if sentences[i] in summary_sentences:\n",
        "        target[i] = 1\n",
        "# Train linear regression model\n",
        "reg = LinearRegression().fit(X, target)\n",
        "\n",
        "# Step 4: Testing\n",
        "# Predict summary sentences\n",
        "y_pred = reg.predict(X)\n",
        "top_sentences = []\n",
        "for i in np.argsort(-y_pred)[:2]:\n",
        "    top_sentences.append(sentences[i])\n",
        "# Generate summary\n",
        "summary = \" \".join(top_sentences)\n",
        "print(summary)\n",
        "\n",
        "# Multiple Linear Regression\n",
        "# Load additional features\n",
        "doc_lengths = np.array([len(doc) for doc in docs])\n",
        "num_sentences = np.array([len(sent_tokenize(doc)) for doc in docs])\n",
        "additional_features = np.column_stack((doc_lengths, num_sentences))\n",
        "\n",
        "# Train multiple linear regression model\n",
        "reg = LinearRegression().fit(np.column_stack((X, additional_features)), target)\n",
        "\n",
        "# Predict summary sentences\n",
        "X_test = vectorizer.transform(sentences).toarray()\n",
        "doc_lengths_test = np.array([len(doc) for doc in docs])\n",
        "num_sentences_test = np.array([len(sent_tokenize(doc)) for doc in docs])\n",
        "additional_features_test = np.column_stack((doc_lengths_test, num_sentences_test))\n",
        "X_test = np.column_stack((X_test, additional_features_test))\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "top_sentences = []\n",
        "for i in np.argsort(-y_pred)[:2]:\n",
        "    top_sentences.append(sentences[i])\n",
        "\n",
        "# Generate summary\n",
        "summary = \" \".join(top_sentences)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4LEZb1fAlNE",
        "outputId": "b6b3a8da-dcf8-49bb-e9fc-42d481352a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First document is about machine learning. Second document is about natural language processing.\n",
            "First document is about machine learning. Second document is about natural language processing.\n"
          ]
        }
      ]
    }
  ]
}